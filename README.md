# RAG_Pipeline_for_LLMs
Large Language Models (LLMs) are powerful, but they have a major limitation: their knowledge is static and limited to the data they were trained on. This is where Retrieval-Augmented Generation (RAG) comes in.

So, What is a RAG Pipeline?
A Retrieval-Augmented Generation (RAG) pipeline consists of two key components:
1. Retriever: Searches a knowledge base for relevant documents based on the userâ€™s query.
2. Generator: Uses retrieved documents as context to generate accurate and relevant responses.
   
RAG improves LLMs by reducing hallucinations through real-world context, ensuring responses are more accurate and grounded in factual information. It also keeps answers up-to-date by retrieving the latest knowledge, eliminating the need for frequent retraining. Additionally, by incorporating external data sources, RAG significantly enhances the factual accuracy of AI-generated responses, which makes LLMs more reliable and context-aware.
   
In the implementation I have :
1.Used Wikipedia as our external knowledge source.
2.Employed Sentence Transformers for embedding text and FAISS for efficient similarity search.
3.Utilized Hugging Faceâ€™s question-answering pipeline to extract answers from retrieved documents.

âš™ï¸ Pipeline Flow:
Wikipedia content â†’ Token chunking â†’ Embedding â†’ FAISS retrieval â†’ Question answering

Highlights:
ğŸ”¹ Disambiguates short/ambiguous terms (e.g., â€œAIâ€ â†’ â€œArtificial Intelligenceâ€)
ğŸ”¹ Token-based chunking with overlap for better context
ğŸ”¹ Embeddings with all-mpnet-base-v2 + FAISS for fast semantic retrieval
ğŸ”¹ QA extraction using RoBERTa from top retrieved chunks
ğŸ”¹ Handles ambiguous queries, providing reliable answers
